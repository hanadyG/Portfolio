{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KraHJ7L9Cc02"
      },
      "source": [
        "# **Import Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --upgrade pip\n",
        "!pip install -q --upgrade jax[cuda11_cudnn805] -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
        "!pip install -q git+https://www.github.com/google/neural-tangents\n",
        "\n",
        "import time\n",
        "from typing import Tuple\n",
        "from absl import app\n",
        "from jax import random\n",
        "import jax.numpy as np\n",
        "import neural_tangents as nt\n",
        "from neural_tangents import stax\n",
        "from examples import datasets\n",
        "from examples import util\n",
        "from IPython.display import set_matplotlib_formats\n",
        "set_matplotlib_formats('pdf', 'svg')\n",
        "import matplotlib\n",
        "import seaborn as sns\n",
        "sns.set(font_scale=1.3)\n",
        "sns.set_style(\"darkgrid\", {\"axes.facecolor\": \".95\"})\n",
        "import matplotlib.pyplot as plt\n",
        "from jax.example_libraries import optimizers\n",
        "from jax import jit, grad, vmap\n",
        "import functools"
      ],
      "metadata": {
        "id": "PvXwzO0qmPHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElJoauTkCiJC"
      },
      "source": [
        "# **Hyperparameters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJOmDKSTilxK"
      },
      "outputs": [],
      "source": [
        "_TRAIN_SIZE = 600  # Dataset size to use for training.\n",
        "_TEST_SIZE = 600  # Dataset size to use for testing.\n",
        "_BATCH_SIZE = 15  # Batch size for kernel computation. 0 for no batching.\n",
        "_MAX_SENTENCE_LENGTH = 500  # Pad/truncate sentences to this length.\n",
        "_GLOVE_PATH = '/content/sample_data/glove.6B.50d.txt'  # Path to GloVe word embeddings.\n",
        "_IMDB_PATH = '/content/sample_data/IMDB'  # Path to imdb sentences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoMvlfRvDR-u"
      },
      "source": [
        "# **Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, y_train, x_test, y_test = datasets.get_dataset(\n",
        "    name='imdb_reviews',\n",
        "    n_train=_TRAIN_SIZE,\n",
        "    n_test=_TEST_SIZE,\n",
        "    do_flatten_and_normalize=False,\n",
        "    data_dir=_IMDB_PATH,\n",
        "    input_key='text')"
      ],
      "metadata": {
        "id": "rlvptNk6mShX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzPM6eEvFdVr"
      },
      "source": [
        "# **Embedding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tD6rK8fxFfnV",
        "outputId": "7d08c139-973d-45c7-a287-0386fa67fabf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading the embedding model\n",
            "Did not find /content/sample_data/glove.6B.50d.txt word embeddings, downloading...\n",
            "Found 400000 word vectors.\n",
            "Found 20714 unique tokens.\n"
          ]
        }
      ],
      "source": [
        "x_train, x_test = datasets.embed_glove(xs=[x_train, x_test],glove_path=_GLOVE_PATH,max_sentence_length=_MAX_SENTENCE_LENGTH,mask_constant=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_ya_ylOFiJZ"
      },
      "source": [
        "# **Model architecture**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model 1**"
      ],
      "metadata": {
        "id": "Uzq1J7-hMX29"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kernel_fn = stax.serial(\n",
        "    stax.Conv(out_chan=1, filter_shape=(9,), strides=(1,), padding='VALID'),\n",
        "    stax.LayerNorm(),\n",
        "    stax.GlobalSelfAttention(\n",
        "        n_chan_out=1,\n",
        "        n_chan_key=1,\n",
        "        n_chan_val=1,\n",
        "        pos_emb_type='SUM',\n",
        "        W_pos_emb_std=1.,\n",
        "        pos_emb_decay_fn=lambda d: 1 / (1 + d**2),\n",
        "        n_heads=2),\n",
        "    stax.Dropout(rate=0.1),\n",
        "    stax.GlobalAvgPool(),\n",
        "    stax.Dropout(rate=0.1),\n",
        "    stax.GlobalAvgPool(),\n",
        "    stax.Dense(out_dim=1),)[2]"
      ],
      "metadata": {
        "id": "q-5ZArnGMeJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model 2**"
      ],
      "metadata": {
        "id": "0d7KfiPSMeb6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kernel_fn = stax.serial(\n",
        "      stax.Conv(out_chan=1, filter_shape=(9,), strides=(1,), padding='VALID'),\n",
        "      stax.GlobalSelfAttention(\n",
        "          n_chan_out=1,\n",
        "          n_chan_key=1,\n",
        "          n_chan_val=1,\n",
        "          pos_emb_type='SUM',\n",
        "          W_pos_emb_std=1.,\n",
        "          pos_emb_decay_fn=lambda d: 1 / (1 + d**2),\n",
        "          n_heads=1),\n",
        "      stax.Relu(),\n",
        "      stax.Dropout(rate=0.1),\n",
        "      stax.GlobalSelfAttention(\n",
        "          n_chan_out=1,\n",
        "          n_chan_key=1,\n",
        "          n_chan_val=1,\n",
        "          pos_emb_type='SUM',\n",
        "          W_pos_emb_std=1.,\n",
        "          pos_emb_decay_fn=lambda d: 1 / (1 + d**2),\n",
        "          n_heads=1),\n",
        "      stax.Relu(),\n",
        "      stax.Dropout(rate=0.1),\n",
        "      stax.GlobalAvgPool(),\n",
        "      stax.Dense(out_dim=1)\n",
        "  )[2]"
      ],
      "metadata": {
        "id": "i21bFjNEz1cw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model 3**"
      ],
      "metadata": {
        "id": "TI4MPT0Xz2Rf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZcQYrkE4xnZS"
      },
      "outputs": [],
      "source": [
        "kernel_fn = stax.serial(\n",
        "      stax.Conv(out_chan=1, filter_shape=(9,), strides=(1,), padding='VALID'),\n",
        "      stax.Relu(),\n",
        "      stax.GlobalSelfAttention(\n",
        "          n_chan_out=1,\n",
        "          n_chan_key=1,\n",
        "          n_chan_val=1,\n",
        "          pos_emb_type='SUM',\n",
        "          W_pos_emb_std=1.,\n",
        "          pos_emb_decay_fn=lambda d: 1 / (1 + d**2),\n",
        "          n_heads=1),\n",
        "      stax.Relu(),\n",
        "      stax.GlobalAvgPool(),\n",
        "      stax.Dense(out_dim=1)\n",
        "  )[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model 4**"
      ],
      "metadata": {
        "id": "vijSFBa0MkCt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kernel_fn = stax.serial(\n",
        "      stax.Conv(out_chan=1, filter_shape=(9,), strides=(1,), padding='VALID'),\n",
        "      stax.Relu(),\n",
        "      stax.GlobalSelfAttention(\n",
        "          n_chan_out=1,\n",
        "          n_chan_key=1,\n",
        "          n_chan_val=1,\n",
        "          pos_emb_type='SUM',\n",
        "          W_pos_emb_std=1.,\n",
        "          pos_emb_decay_fn=lambda d: 1 / (1 + d**2),\n",
        "          n_heads=1),\n",
        "      stax.Relu(),\n",
        "      stax.GlobalSelfAttention(\n",
        "          n_chan_out=1,\n",
        "          n_chan_key=1,\n",
        "          n_chan_val=1,\n",
        "          pos_emb_type='SUM',\n",
        "          W_pos_emb_std=1.,\n",
        "          pos_emb_decay_fn=lambda d: 1 / (1 + d**2),\n",
        "          n_heads=1),\n",
        "      stax.Relu(),\n",
        "            stax.GlobalSelfAttention(\n",
        "          n_chan_out=1,\n",
        "          n_chan_key=1,\n",
        "          n_chan_val=1,\n",
        "          pos_emb_type='SUM',\n",
        "          W_pos_emb_std=1.,\n",
        "          pos_emb_decay_fn=lambda d: 1 / (1 + d**2),\n",
        "          n_heads=1),\n",
        "      stax.Relu(),\n",
        "      stax.GlobalAvgPool(),\n",
        "      stax.Dense(out_dim=1)\n",
        "  )[2]"
      ],
      "metadata": {
        "id": "3hvD0Yk7Mi8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ku0eOkbGVUy"
      },
      "source": [
        "# **Training**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "kernel_fn = nt.batch(kernel_fn, device_count=-1, batch_size=_BATCH_SIZE)\n",
        "start = time.time()\n",
        "predict = nt.predict.gradient_descent_mse_ensemble(\n",
        "    kernel_fn=kernel_fn,\n",
        "    x_train=x_train,\n",
        "    y_train=y_train,\n",
        "    diag_reg=1e-6,\n",
        "    mask_constant=0)\n",
        "fx_test_nngp, fx_test_ntk = predict(x_test=x_test, get=('nngp', 'ntk'))\n",
        "fx_test_nngp.block_until_ready()\n",
        "fx_test_ntk.block_until_ready()\n",
        "duration = time.time() - start\n",
        "print(f'Kernel construction and inference done in {duration} seconds.')\n",
        "loss = lambda fx, y_hat: 0.5 * np.mean((fx - y_hat) ** 2)\n",
        "util.print_summary('NNGP test', y_test, fx_test_nngp, None, loss)\n",
        "util.print_summary('NTK test', y_test, fx_test_ntk, None, loss)"
      ],
      "metadata": {
        "id": "-ZXQ1Ij4mNFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKb91-OfUYUB"
      },
      "source": [
        "# **Phase diagram**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWMU1YkS5lPB"
      },
      "source": [
        "### **Imports & Utils**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-U2XtTJ-kma4"
      },
      "outputs": [],
      "source": [
        "!pip install -q --upgrade pip\n",
        "!pip install -q --upgrade jax[cuda11_cudnn805] -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
        "!pip install -q git+https://www.github.com/google/neural-tangents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1nTu8wtiK5w"
      },
      "outputs": [],
      "source": [
        "import jax.numpy as np\n",
        "\n",
        "from jax.example_libraries import optimizers\n",
        "from jax import grad, jit, vmap\n",
        "from jax import lax\n",
        "from jax.config import config\n",
        "config.update('jax_enable_x64', True)\n",
        "\n",
        "from functools import partial\n",
        "\n",
        "import neural_tangents as nt\n",
        "from neural_tangents import stax\n",
        "\n",
        "_Kernel = nt._src.utils.kernel.Kernel\n",
        "\n",
        "def Kernel(K):\n",
        "  \"\"\"Create an input Kernel object out of an np.ndarray.\"\"\"\n",
        "  return _Kernel(cov1=np.diag(K), nngp=K, cov2=None, \n",
        "                 ntk=None, is_gaussian=True, is_reversed=False,\n",
        "                 diagonal_batch=True, diagonal_spatial=False,\n",
        "                 shape1=(K.shape[0], 1024), shape2=(K.shape[1], 1024),\n",
        "                 x1_is_x2=True, is_input=True, batch_axis=0, channel_axis=1,\n",
        "                 mask1=None, mask2=None) \n",
        "  \n",
        "def fixed_point(f, initial_value, threshold):\n",
        "  \"\"\"Find fixed-points of a function f:R->R using Newton's method.\"\"\"\n",
        "  g = lambda x: f(x) - x\n",
        "  dg = grad(g)\n",
        "\n",
        "  def cond_fn(x):\n",
        "    x, last_x = x\n",
        "    return np.abs(x - last_x) > threshold\n",
        "\n",
        "  def body_fn(x):\n",
        "    x, _ = x\n",
        "    return x - g(x) / dg(x), x\n",
        "  \n",
        "  return lax.while_loop(cond_fn, body_fn, (initial_value, 0.0))[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjtNH1OL7vl8"
      },
      "outputs": [],
      "source": [
        "from IPython.display import set_matplotlib_formats\n",
        "set_matplotlib_formats('pdf', 'svg')\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_style(style='white')\n",
        "\n",
        "def format_plot(x='', y='', grid=True):  \n",
        "  ax = plt.gca()\n",
        "    \n",
        "  plt.grid(grid)\n",
        "  plt.xlabel(x, fontsize=20)\n",
        "  plt.ylabel(y, fontsize=20)\n",
        "  \n",
        "def finalize_plot(shape=(1, 1)):\n",
        "  plt.gcf().set_size_inches(\n",
        "    shape[0] * 1.5 * plt.gcf().get_size_inches()[1], \n",
        "    shape[1] * 1.5 * plt.gcf().get_size_inches()[1])\n",
        "  plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MF_hLTxuiHbD"
      },
      "source": [
        "### **Phase Diagram**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNS7Qrb7LHY2"
      },
      "outputs": [],
      "source": [
        "def c_map(W_var, b_var):\n",
        "  W_std = np.sqrt(W_var)\n",
        "  b_std = np.sqrt(b_var)\n",
        "\n",
        "  # Create a single layer of a network as an affine transformation composed\n",
        "  # with an Erf nonlinearity.\n",
        "  kernel_fn = stax.GlobalSelfAttention(\n",
        "        n_chan_out=1,\n",
        "        n_chan_key=1,\n",
        "        n_chan_val=1,\n",
        "        pos_emb_type='SUM',\n",
        "        W_pos_emb_std=1.,\n",
        "        pos_emb_decay_fn=lambda d: 1 / (1 + d**2),\n",
        "        n_heads=2,W_out_std=W_std,b_std= b_std)[2]\n",
        "\n",
        "  def q_map_fn(q):\n",
        "    return kernel_fn(Kernel(np.array([[q]]))).nngp[0, 0]\n",
        "  \n",
        "  qstar = fixed_point(q_map_fn, 1.0, 1e-7)\n",
        "\n",
        "  def c_map_fn(c):\n",
        "    K = np.array([[qstar, qstar * c], [qstar * c, qstar]])\n",
        "    K_out = kernel_fn(Kernel(K)).nngp\n",
        "    return K_out[1, 0] / qstar\n",
        "\n",
        "  return c_map_fn\n",
        "\n",
        "c_star = lambda W_var, b_var: fixed_point(c_map(W_var, b_var), 0.1, 1e-7)\n",
        "chi = lambda c, W_var, b_var: grad(c_map(W_var, b_var))(c)\n",
        "chi_1 = partial(chi, 1.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zyZhNMBnUJbC"
      },
      "outputs": [],
      "source": [
        "def vectorize_over_sw_sb(fn):\n",
        "  # Vectorize over the weight variance.\n",
        "  fn = vmap(fn, (0, None))\n",
        "  # Vectorize over the bias variance.\n",
        "  fn = vmap(fn, (None, 0))\n",
        "\n",
        "  return fn\n",
        "\n",
        "c_star = jit(vectorize_over_sw_sb(c_star))\n",
        "chi_1 = jit(vectorize_over_sw_sb(chi_1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9x74oakFjSk"
      },
      "outputs": [],
      "source": [
        "W_var = np.arange(0, 3, 0.01)\n",
        "b_var = np.arange(0., 0.25, 0.001)\n",
        "\n",
        "plt.contourf(W_var, b_var, c_star(W_var, b_var))\n",
        "plt.colorbar()\n",
        "plt.title('$C^*$ as a function of weight and bias variance', fontsize=14)\n",
        "\n",
        "format_plot('$\\\\sigma_w^2$', '$\\\\sigma_b^2$')\n",
        "finalize_plot((1.15, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cr7MFuCs1Y-3"
      },
      "outputs": [],
      "source": [
        "plt.contourf(W_var, b_var, c_star(W_var, b_var) > 0.999, \n",
        "             levels=3, \n",
        "             colors=[[1.0, 0.89, 0.811], [0.85, 0.85, 1]])\n",
        "plt.title('Phase diagram in terms of weight and bias variance', fontsize=14)\n",
        "\n",
        "format_plot('$\\\\sigma_w^2$', '$\\\\sigma_b^2$')\n",
        "finalize_plot((1, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4e4j-wvO315"
      },
      "outputs": [],
      "source": [
        "plt.contourf(W_var, b_var, chi_1(W_var, b_var))\n",
        "plt.colorbar()\n",
        "plt.title(r'$\\chi^1$ as a function of weight and bias variance', fontsize=14)\n",
        "\n",
        "format_plot('$\\\\sigma_w^2$', '$\\\\sigma_b^2$')\n",
        "finalize_plot((1.15, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4SgMiPXPNhl"
      },
      "outputs": [],
      "source": [
        "plt.contourf(W_var, b_var, c_star(W_var, b_var) > 0.999, \n",
        "             levels=3, \n",
        "             colors=[[1.0, 0.89, 0.811], [0.85, 0.85, 1]])\n",
        "plt.contourf(W_var, b_var, \n",
        "             np.abs(chi_1(W_var, b_var) - 1) < 0.003, \n",
        "             levels=[0.5, 1], \n",
        "             colors=[[0, 0, 0]])\n",
        "\n",
        "plt.title('Phase diagram in terms of weight and bias variance', fontsize=14)\n",
        "\n",
        "format_plot('$\\\\sigma_w^2$', '$\\\\sigma_b^2$')\n",
        "finalize_plot((1, 1))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}